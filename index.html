<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Tianfang  Zhang</title>
    <meta name="author" content="Tianfang  Zhang">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tianfang-zhang.github.io/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Tianfang  <span class="font-weight-bold">Zhang</span>
          </h1>
          <p class="desc"><b>Algorithm Researcher</b> @ <a href="https://www.sensetime.com/cn" rel="external nofollow noopener" target="_blank">SenseTime</a> and <b>Post Doc</b> @ <a href="https://www.tsinghua.edu.cn/" rel="external nofollow noopener" target="_blank">THU</a></p>
        </header>

        <article>
          <div class="profile float-right" style="max-width: 30vw">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_image-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_image-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_image-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile_image.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="profile_image.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              
            </div>

            <!-- Social -->
              <div class="social">
                <div class="contact-icons">
                  <a href="mailto:%73%70%61%72%6B%63%61%72%6C%65%74%6F%6E@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-4183-7053" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=ROZkiXQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Tianfang-Zhang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            

                </div>

                <div class="contact-note">
                  The best way to reach me is by email.

                </div>

              </div>

          </div>

          <div class="clearfix">
            <p>Hey, thanks for stopping by! ðŸ‘‹</p>

<p>I am working as an algorithm researcher at <a href="https://www.sensetime.com/cn" rel="external nofollow noopener" target="_blank">SenseTime</a> and a post doc at <a href="https://www.tsinghua.edu.cn/" rel="external nofollow noopener" target="_blank">THU</a>. My current research interests includes lightweight network design, multimodal large language models, diffusion models.</p>

<!-- I am pursuing a PhD at [the Laboratory of Imaging Detection and Intelligent Perecption (IDIP Lab)](http://idiplab.uestc.cn/) of [University of Electronic Science and Technology of China (UESTC)](https://www.uestc.edu.cn/), supresived by [Prof. Zhenming Peng](https://www.sice.uestc.edu.cn/info/1302/5086.htm) -->

<!-- My PhD research is dedicated to developing trustworthy infrared small target detection methods and also extended to sparse representation, optimization, deep learning and deep unfolding networks. In collaboration with my colleagues, my research interests also include segmentation and image compressive sensing. -->

<p>Previously, I received my PhD degree from <a href="https://www.uestc.edu.cn/" rel="external nofollow noopener" target="_blank">UESTC</a> (2023), supresived by <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Prof. Zhenming Peng</a>, and worked at <a href="http://idiplab.uestc.cn/" rel="external nofollow noopener" target="_blank">the Laboratory of Imaging Detection and Intelligent Perecption (IDIP Lab)</a>. My PhD research is dedicated to developing trustworthy infrared small target detection methods and also extended to sparse representation, optimization, deep learning and deep unfolding networks.</p>

<p>Over the past few years, I had the pleasure of being funded by <a href="https://www.csc.edu.cn/" rel="external nofollow noopener" target="_blank">China Scholarship Council (CSC)</a> and working at the <a href="https://di.ku.dk/english/research/imagesection/" rel="external nofollow noopener" target="_blank">Image Section</a> of <a href="https://di.ku.dk/english/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a>, <a href="https://www.ku.dk/english/" rel="external nofollow noopener" target="_blank">University of Copenhagen</a> as a visiting PhD, supervised by <a href="https://sporring.github.io/" rel="external nofollow noopener" target="_blank">Prof. Jon Sporring</a>.</p>

<!-- Previously, I received a Bachelorâ€™s degree from [UESTC](https://www.uestc.edu.cn/) (2017). And I had the pleasure of working at the [Image Section](https://di.ku.dk/english/research/imagesection/) of [Department of Computer Science](https://di.ku.dk/english/), [University of Copenhagen](https://www.ku.dk/english/) as a visiting PhD, supervised by [Prof. Jon Sporring](https://sporring.github.io/) and funded by [China Scholarship Council (CSC)](https://www.csc.edu.cn/). -->

          </div>

          <!-- News -->
          <!-- <h2><a href="/news/" style="color: inherit;">news</a></h2> -->          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row" class="th-sm">09 Oct, 2024</th>
                  <td>
                    <a href="https://github.com/Tianfang-Zhang/CAS-ViT" rel="external nofollow noopener" target="_blank">Code and pretrained weights</a> of our work on lightweight vision transformers named as <a href="https://arxiv.org/abs/2408.03703" rel="external nofollow noopener" target="_blank">CAS-ViT</a> were accessible.

                  </td>
                </tr>
                <tr>
                  <th scope="row" class="th-sm">08 Nov, 2023</th>
                  <td>
                    ðŸŽ‰ðŸŽ‰ðŸŽ‰ Our work focus on infrared small target detection via deep unfolding network titled <a href="https://arxiv.org/abs/2311.00917" rel="external nofollow noopener" target="_blank">RPCANet</a> was accpeted by <a href="https://wacv2024.thecvf.com/" rel="external nofollow noopener" target="_blank">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2024</a>.

                  </td>
                </tr>
                <tr>
                  <th scope="row" class="th-sm">03 Oct, 2023</th>
                  <td>
                    Code and pre-trained model of <a href="https://www.sciencedirect.com/science/article/abs/pii/S095070512300713X" rel="external nofollow noopener" target="_blank">CT-Net</a> and <a href="https://ieeexplore.ieee.org/abstract/document/10065722" rel="external nofollow noopener" target="_blank">LR-CSNet</a> are released in <a href="https://github.com/Tianfang-Zhang/CT-Net" rel="external nofollow noopener" target="_blank">Github CT-Net</a> and <a href="https://github.com/Tianfang-Zhang/LowRank-CSNet" rel="external nofollow noopener" target="_blank">Github LR-CSNet</a>.

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <!-- <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> -->
          <div class="publications">
            <h2>Selected Publications</h2>
            <p>Find the full list <a href="publications">here</a>. Authors marked by * contributed equally.</p>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr"><abbr class="badge">Arxiv</abbr></div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/casvit.png">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="zhang2024cas" class="col-sm-8">
        <!-- Title -->
        <div class="title">CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications</div>
        <!-- Author -->
        <div class="author">
        

        <em>Tianfang Zhang</em>,Â <a href="https://di.ku.dk/english/staff/?pure=en/persons/702491" rel="external nofollow noopener" target="_blank">Lei Li</a>,Â Yang Zhou,Â Wentao Liu,Â Chen Qian,Â andÂ Xiangyang Ji</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em></em> 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="http://arxiv.org/abs/2408.03703" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Tianfang-Zhang/CAS-ViT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixerâ€™s powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we construct a novel additive similarity function following this paradigm and present an efficient implementation named Convolutional Additive Token Mixer (CATM). This simplification leads to a significant reduction in computational overhead. We evaluate CAS-ViT across a variety of vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. Our experiments, conducted on GPUs, ONNX, and iPhones, demonstrate that CAS-ViT achieves a competitive performance when compared to other state-of-the-art backbones, establishing it as a viable option for efficient mobile vision applications.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr"><abbr class="badge">WACV 2024</abbr></div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/rpcanet.png">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="wu2023rpcanet" class="col-sm-8">
        <!-- Title -->
        <div class="title">RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection</div>
        <!-- Author -->
        <div class="author">
        

        Fengyi Wu*,Â <em>Tianfang Zhang*</em>,Â <a href="https://di.ku.dk/english/staff/?pure=en/persons/702491" rel="external nofollow noopener" target="_blank">Lei Li</a>,Â Yian Huang,Â andÂ <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Zhenming Peng</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="http://arxiv.org/abs/2311.00917" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/fengyiwu98/RPCANet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Deep learning (DL) networks have achieved remarkable performance in infrared small target detection (ISTD). However, these structures exhibit a deficiency in interpretability and are widely regarded as black boxes, as they disregard domain knowledge in ISTD. To alleviate this issue, this work proposes an interpretable deep network for detecting infrared dim targets, dubbed RPCANet. Specifically, our approach formulates the ISTD task as sparse target extraction, low-rank background estimation, and image reconstruction in a relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the iterative optimization updating steps into a deep-learning framework, time-consuming and complex matrix calculations are replaced by theory-guided neural networks. RPCANet detects targets with clear interpretability and preserves the intrinsic image feature, instead of directly transforming the detection task into a matrix decomposition problem. Extensive experiments substantiate the effectiveness of our deep unfolding framework and demonstrate its trustworthy results, surpassing baseline methods in both qualitative and quantitative evaluations.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr"><abbr class="badge">KBS</abbr></div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ctnet.png">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="zhang2023optimization" class="col-sm-8">
        <!-- Title -->
        <div class="title">Optimization-inspired Cumulative Transmission Network for image compressive sensing</div>
        <!-- Author -->
        <div class="author">
        

        <em>Tianfang Zhang*</em>,Â <a href="https://di.ku.dk/english/staff/?pure=en/persons/702491" rel="external nofollow noopener" target="_blank">Lei Li*</a>,Â andÂ <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Zhenming Peng</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Knowledge-Based Systems</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S095070512300713X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="https://github.com/Tianfang-Zhang/CT-Net" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Compressive Sensing (CS) techniques enable accurate signal reconstruction with few measurements. Deep Unfolding Networks (DUNs) have recently been shown to increase the efficiency of CS by emulating iterative CS optimization procedures by neural networks. However, most of these DUNs suffer from redundant update procedures or complex matrix operations, which can impair their reconstruction performances. Here we propose the optimization-inspired Cumulative Transmission Network (CT-Net), a DUN approach for natural image CS. We formulate an optimization procedure introducing an auxiliary variable similar to Half Quadratic Splitting (HQS). Unfolding this procedure defines the basic structure of our neural architecture, which is then further refined. A CT-Net is composed of Reconstruction Fidelity Modules (RFMs) for minimizing the reconstruction error and Constraint Gradient Approximation (CGA) modules for approximating (the gradient of) sparsity constraints instead of relying on an analytic solutions such as soft-thresholding. Furthermore, a lightweight Cumulative Transmission (CT) between CGAs in each reconstruction stage is proposed to facilitate a better feature representation. Experiments on several widely used natural image benchmarks illustrate the effectiveness of CT-Net with significant performance improvements and fewer network parameters compared to existing state-of-the-art methods. The experiments also demonstrate the scene and noise robustness of the proposed method.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr"><abbr class="badge"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7" rel="external nofollow noopener" target="_blank">TAES</a></abbr></div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/agpcnet.jpg">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="zhang2023attention" class="col-sm-8">
        <!-- Title -->
        <div class="title">Attention-Guided Pyramid Context Networks for Detecting Infrared Small Target Under Complex Background</div>
        <!-- Author -->
        <div class="author">
        

        <em>Tianfang Zhang</em>,Â <a href="https://di.ku.dk/english/staff/?pure=en/persons/702491" rel="external nofollow noopener" target="_blank">Lei Li</a>,Â Siying Cao,Â Tian Pu,Â andÂ <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Zhenming Peng</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Aerospace and Electronic Systems</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10024907" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="https://github.com/Tianfang-Zhang/AGPCNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Infrared small target detection techniques remain a challenging task due to the complex background. To overcome this problem, by exploring context information, this research presents a data-driven approach called Attention-Guided Pyramid Context Network (AGPCNet). Specifically, we design Attention-Guided Context Block (AGCB) and perceive pixel correlations within and between patches at specific scales via Local Semantic Association (LSA) and Global Context Attention (GCA) respectively. Then the contextual information from multiple scales is fused by Context Pyramid Module (CPM) to achieve better feature representation. In the upsampling stage, we fuse the low and deep semantics through Asymmetric Fusion Module (AFM) to retain more information about small targets. The experimental results illustrate that AGPCNet has achieved state-of-the-art performance on three available infrared small target datasets.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr">
<abbr class="badge">ICCC 2022</abbr>
                <span class="badge award">Oral</span>
</div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/lrcsnet.png">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="zhang2022lrcsnet" class="col-sm-8">
        <!-- Title -->
        <div class="title">LR-CSNet: Low-rank Deep Unfolding Network for Image Compressive Sensing</div>
        <!-- Author -->
        <div class="author">
        

        <em>Tianfang Zhang</em>,Â <a href="https://di.ku.dk/english/staff/?pure=en/persons/702491" rel="external nofollow noopener" target="_blank">Lei Li</a>,Â <a href="https://christian-igel.github.io/" rel="external nofollow noopener" target="_blank">Christian Igel</a>,Â Stefan Oehmcke,Â Fabian Gieseke,Â andÂ <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Zhenming Peng</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 8th IEEE International Conference on Computer and Communications</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="http://arxiv.org/abs/2212.09088" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="http://www-engineeringvillage-com-s.vpn.uestc.edu.cn:8118/app/doc/?docid=cpx_M183ebca618790c4a270M7b8f10178165143&amp;pageSize=25&amp;index=1&amp;searchId=d5a68db9634b471d90ae83ce0532635a&amp;resultsCount=2&amp;usageZone=resultslist&amp;usageOrigin=searchresults&amp;searchType=Quick" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="https://github.com/Tianfang-Zhang/LowRank-CSNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Deep unfolding networks (DUNs) have proven to be a viable approach to compressive sensing (CS). In this work, we propose a DUN called low-rank CS network (LR-CSNet) for natural image CS. Real-world image patches are often well-represented by low-rank approximations. LR-CSNet exploits this property by adding a low-rank prior to the CS optimization task. We derive a corresponding iterative optimization procedure using variable splitting, which is then translated to a new DUN architecture. The architecture uses low-rank generation modules (LRGMs), which learn low-rank matrix factorizations, as well as gradient descent and proximal mappings (GDPMs), which are proposed to extract high-frequency features to refine image details. In addition, the deep features generated at each reconstruction stage in the DUN are transferred between stages to boost the performance. Our extensive experiments on three widely considered datasets demonstrate the promising performance of LR-CSNet compared to state-of-the-art methods in natural image CS.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr"><abbr class="badge"><a href="https://www.sciencedirect.com/journal/neurocomputing" rel="external nofollow noopener" target="_blank">Neurocomputing</a></abbr></div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/srws.jpg">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="zhang2021infrared" class="col-sm-8">
        <!-- Title -->
        <div class="title">Infrared small target detection via self-regularized weighted sparse model</div>
        <!-- Author -->
        <div class="author">
        

        <em>Tianfang Zhang</em>,Â <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Zhenming Peng</a>,Â Hao Wu,Â Yanmin He,Â Chaohai Li,Â andÂ Chunping Yang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Neurocomputing</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0925231220313461" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="https://github.com/Tianfang-Zhang/SRWS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Infrared search and track (IRST) system is widely used in many fields, however, itâ€™s still a challenging task to detect infrared small targets in complex background. This paper proposed a novel detection method called self-regularized weighted sparse (SRWS) model. The algorithm is designed for the hypothesis that data may come from multi-subspaces. And the overlapping edge information (OEI), which can detect the background structure information, is applied to constrain the sparse item and enhance the accuracy. Furthermore, the self-regularization item is applied to mine the potential information in background, and extract clutter from multi-subspaces. Therefore, the infrared small target detection problem is transformed into an optimization problem. By combining the optimization function with alternating direction method of multipliers (ADMM), we explained the solution method of SRWS and optimized its iterative convergence condition. A series of experimental results show that the proposed method outperforms state-of-the-art baselines.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 no-gutters">
<div class="row no-gutters">
            <div class="col abbr"><abbr class="badge"><a href="https://www.mdpi.com/journal/remotesensing" rel="external nofollow noopener" target="_blank">Remote Sens.</a></abbr></div>
          </div>
<!-- <div class="w-100"></div> --><div class="row no-gutters">
            <div class="col preview">
              <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/nolc.png">
</div>
          </div>
</div>

        <!-- Entry bib key -->
        <div id="zhang2019infrared" class="col-sm-8">
        <!-- Title -->
        <div class="title">Infrared small target detection based on non-convex optimization with Lp-norm constraint</div>
        <!-- Author -->
        <div class="author">
        

        <em>Tianfang Zhang</em>,Â Hao Wu,Â <a href="https://scholar.google.com/citations?user=Rmsc214AAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Yuhan Liu</a>,Â Lingbing Peng,Â Chunping Yang,Â andÂ <a href="https://www.sice.uestc.edu.cn/info/1302/5086.htm" rel="external nofollow noopener" target="_blank">Zhenming Peng</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Remote Sensing</em>, 2019
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a>
            <a href="https://www.mdpi.com/2072-4292/11/5/559" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="https://github.com/Tianfang-Zhang/NOLC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The infrared search and track (IRST) system has been widely used, and the field of infrared small target detection has also received much attention. Based on this background, this paper proposes a novel infrared small target detection method based on non-convex optimization with Lp-norm constraint (NOLC). The NOLC method strengthens the sparse item constraint with Lp-norm while appropriately scaling the constraints on low-rank item, so the NP-hard problem is transformed into a non-convex optimization problem. First, the infrared image is converted into a patch image and is secondly solved by the alternating direction method of multipliers (ADMM). In this paper, an efficient solver is given by improving the convergence strategy. The experiment shows that NOLC can accurately detect the target and greatly suppress the background, and the advantages of the NOLC method in detection efficiency and computational efficiency are verified.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          
          
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2024 Tianfang  Zhang. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
